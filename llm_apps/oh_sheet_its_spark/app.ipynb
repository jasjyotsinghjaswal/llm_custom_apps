{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"{project_path}\\llm_custom_apps\\llm_apps\\oh_sheet_its_spark\\utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"\\nYou are a helpful assistant to a Data Scientist who is working on a project to transform Excel Spreadsheets to PySpark Dataframes. \\nThe Data Scientist has provided you with a dictionary of dictionaries which contains details about Excel spreadsheets.\\nEach dictionary has only one key which is the name of the Sheet and the value is a dictionary with multiple keys.\\nEach key is the name of the Column Header in the Excel Spreadsheet. The value is again a dictionary with two keys.\\nThe first key is 'ColumnID' with the value being the associated ColumnID in Excel i.e. A, B, C, or etc.\\nThe second key is 'ColumnValue' with the value being the associated formula for generating the column or the hardcoded value in the absence of the formula.\\nThe dictionary looks something like:\\n\\n{{'SheetNum1': {'X': {'ColumnID': 'A', 'ColumnValue': 11},\\n   'Y': {'ColumnID': 'B', 'ColumnValue': 2020},\\n   'Z': {'ColumnID': 'C', 'ColumnValue': 15789},\\n   'P': {'ColumnID': 'D', 'ColumnValue': 'KIO'},\\n   'Q': {'ColumnID': 'E', 'ColumnValue': 'SECCC'}}},\\n {'SheetNum2': {'ID': {'ColumnID': 'A', 'LLOP': 1},\\n   'Location': {'ColumnID': 'B', 'ColumnValue': 'LOPP'}}}}\\n\\nYou have to convert the spreadsheet transformation logic to pyspark dataframe keeping the following in mind\\n\\n1) Parsing the dictionary figure out the dependency between sheets and between columns to identify the order in which the transformations need to be defined and the columns that are hardcoded.\\n2) In your response at the beginning provide explanation about the Sheets that are present.For each Sheet specify what columns are hardcoded and what columns are derived .Do not specify the values of the columns but only the column names.\\n3) Now respond with code with the code block starting with ```python and ending with ``` . Ensure:\\n  a) All the python dependencies that will be required to accomplish this task are imported and after declaring dependencies, also create a placeholder to read the spreadsheet creating multiple pyspark dataframes , one for each sheet in the spreadsheet if and only if it has atleast one hardcoded column (i.e. not derived using formula) and only read/select the hardcoded columns from dataframe.Rememeber I want you to create Dataframes from the individual sheets of spreadsheet and not creating them explictly from the hardcoded values in the dictionary provided\\n  b) Now create new dataframes from the dataframes declared as input and derive transformed columns for each of the column that is derived using formulas based on dictionary provided , with column name in pyspark being same as Column Header provided in the dictionary.\\n  c) Against each transformation (withColumn,agg methods, join etc. ) always have in comments the Psuedo Code(Logic in plain english used to derive the column.) on the same line .\\n  d) Subsequently the user might ask for modications for removing /adding /modifying existing transformations to which you should comply accordingly by updating entire code but keep explanation short and bare minimum.\\n\\n\\n\\n\"}, {'role': 'user', 'content': 'Convert the excel logic representation to equivalent pyspark code : {\\'SalesInfo\\': {\\'StoreID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'11\\'}, \\'SalesYear\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'2020\\'}, \\'SalesValue\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'15789\\'}, \\'StoreLocation\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\")\\'}, \\'Owner\\': {\\'ColumnID\\': \\'E\\', \\'ColumnValue\\': \\'=IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\")\\'}}, \\'RegionInfo\\': {\\'ID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'1\\'}, \\'Location\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'Halifax\\'}}, \\'StoreInfo\\': {\\'ID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'11\\'}, \\'RegionID\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'1\\'}, \\'StoreOwner\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'Ethan Carter\\'}}, \\'SaleGrowthReport1\\': {\\'StoreID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'=_xlfn.UNIQUE(_xlfn._xlws.FILTER(SalesInfo!A:A, SalesInfo!A:A<>\"\"))\\'}, \\'AverageSales\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'=AVERAGEIF(SalesInfo!A:A, A2, SalesInfo!C:C)\\'}, \\'MinSales\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'=MIN(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\'}, \\'MaxSales\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=MAX(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\'}}, \\'SaleGrowthReport2\\': {\\'StoreLocation-SalesYear\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'=_xlfn.UNIQUE(SalesInfo!D:D & \"-\" & SalesInfo!B:B)\\'}, \\'Year\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'=RIGHT(A2, LEN(A2) - FIND(\"-\", A2))\\'}, \\'Location\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'=LEFT(A2, FIND(\"-\", A2) - 1)\\'}, \\'AverageSales\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=AVERAGEIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}, \\'MinSales\\': {\\'ColumnID\\': \\'E\\', \\'ColumnValue\\': \\'=_xlfn.MINIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}, \\'MaxSales\\': {\\'ColumnID\\': \\'F\\', \\'ColumnValue\\': \\'=_xlfn.MAXIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}}}'}, {'role': 'assistant', 'content': '### Explanation:\\n\\n#### Sheets and Columns:\\n1. **SalesInfo**:\\n   - **Hardcoded Columns**: StoreID, SalesYear, SalesValue\\n   - **Derived Columns**: StoreLocation, Owner\\n\\n2. **RegionInfo**:\\n   - **Hardcoded Columns**: ID, Location\\n   - **Derived Columns**: None\\n\\n3. **StoreInfo**:\\n   - **Hardcoded Columns**: ID, RegionID, StoreOwner\\n   - **Derived Columns**: None\\n\\n4. **SaleGrowthReport1**:\\n   - **Hardcoded Columns**: None\\n   - **Derived Columns**: StoreID, AverageSales, MinSales, MaxSales\\n\\n5. **SaleGrowthReport2**:\\n   - **Hardcoded Columns**: None\\n   - **Derived Columns**: StoreLocation-SalesYear, Year, Location, AverageSales, MinSales, MaxSales\\n\\n### Pyspark Code:\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, avg, min, max, when, substring_index, length, concat_ws, lit\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"ExcelToPySpark\").getOrCreate()\\n\\n# Read Excel sheets into DataFrames if they contain at least one hardcoded column\\nsales_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"SalesInfo!A1:D10\").load().selectExpr(\"A as StoreID\", \"B as SalesYear\", \"C as SalesValue\")\\nregion_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"RegionInfo!A1:B10\").load().selectExpr(\"A as ID\", \"B as Location\")\\nstore_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"StoreInfo!A1:C10\").load().selectExpr(\"A as ID\", \"B as RegionID\", \"C as StoreOwner\")\\n\\n# Derive columns for SalesInfo using RegionInfo and StoreInfo\\nsales_info_df = sales_info_df.join(store_info_df, sales_info_df.StoreID == store_info_df.ID, \"left\") \\\\\\n                             .join(region_info_df, store_info_df.RegionID == region_info_df.ID, \"left\") \\\\\\n                             .withColumn(\"StoreLocation\", when(region_info_df.Location.isNotNull(), region_info_df.Location).otherwise(lit(\"\"))) \\\\\\n                             .withColumn(\"Owner\", when(store_info_df.StoreOwner.isNotNull(), store_info_df.StoreOwner).otherwise(lit(\"\"))) \\\\\\n                             .drop(store_info_df.ID, store_info_df.RegionID, store_info_df.StoreOwner, region_info_df.ID)\\n\\n# Derive columns for SaleGrowthReport1 using SalesInfo\\nsale_growth_report1_df = sales_info_df.groupBy(\"StoreID\") \\\\\\n                                      .agg(avg(col(\"SalesValue\")).alias(\"AverageSales\"), \\\\\\n                                           min(col(\"SalesValue\")).alias(\"MinSales\"), \\\\\\n                                           max(col(\"SalesValue\")).alias(\"MaxSales\"))\\n\\n# Derive columns for SaleGrowthReport2 using SalesInfo\\nsale_growth_report2_df = sales_info_df.withColumn(\"StoreLocation-SalesYear\", concat_ws(\"-\", col(\"StoreLocation\"), col(\"SalesYear\"))) \\\\\\n                                      .select(\"StoreLocation-SalesYear\", \"SalesYear\", \"StoreLocation\") \\\\\\n                                      .distinct() \\\\\\n                                      .withColumn(\"Year\", substring_index(col(\"StoreLocation-SalesYear\"), \"-\", -1)) \\\\\\n                                      .withColumn(\"Location\", substring_index(col(\"StoreLocation-SalesYear\"), \"-\", 1)) \\\\\\n                                      .alias(\"temp_df\") \\\\\\n                                      .join(sales_info_df.alias(\"si\"), (col(\"temp_df.Location\") == col(\"si.StoreLocation\")) & (col(\"temp_df.Year\") == col(\"si.SalesYear\")), \"inner\") \\\\\\n                                      .groupBy(\"temp_df.StoreLocation-SalesYear\", \"temp_df.Year\", \"temp_df.Location\") \\\\\\n                                      .agg(avg(col(\"si.SalesValue\")).alias(\"AverageSales\"), \\\\\\n                                           min(col(\"si.SalesValue\")).alias(\"MinSales\"), \\\\\\n                                           max(col(\"si.SalesValue\")).alias(\"MaxSales\"))\\n```\\n\\n### Notes:\\n- The `spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"SheetName!A1:D10\").load()` is a placeholder for reading Excel sheets. The actual implementation may vary based on the availability of Excel readers like `pyspark-excel` or `spark-excel`.\\n- The actual column ranges (`A1:D10`) should be adjusted based on the actual data.\\n- The derived columns are created using appropriate PySpark functions and transformations.\\n- This code assumes that the Excel sheets are readable and properly formatted as per the provided dictionary.'}, {'role': 'User', 'content': 'Can you make 1 small change'}, {'role': 'assistant', 'content': '### Explanation:\\n\\n#### Sheets and Columns:\\n1. **SalesInfo**:\\n   - **Hardcoded Columns**: StoreID, SalesYear, SalesValue\\n   - **Derived Columns**: StoreLocation, Owner\\n\\n2. **RegionInfo**:\\n   - **Hardcoded Columns**: ID, Location\\n   - **Derived Columns**: None\\n\\n3. **StoreInfo**:\\n   - **Hardcoded Columns**: ID, RegionID, StoreOwner\\n   - **Derived Columns**: None\\n\\n4. **SaleGrowthReport1**:\\n   - **Hardcoded Columns**: None\\n   - **Derived Columns**: StoreID, AverageSales, MinSales, MaxSales\\n\\n5. **SaleGrowthReport2**:\\n   - **Hardcoded Columns**: None\\n   - **Derived Columns**: StoreLocation-SalesYear, Year, Location, AverageSales, MinSales, MaxSales\\n\\n### Pyspark Code:\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, avg, min, max, when, substring_index, length, concat_ws, lit\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"ExcelToPySpark\").getOrCreate()\\n\\n# Read Excel sheets into DataFrames if they contain at least one hardcoded column\\nsales_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"SalesInfo!A1:C10\").load().selectExpr(\"A as StoreID\", \"B as SalesYear\", \"C as SalesValue\")\\nregion_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"RegionInfo!A1:B10\").load().selectExpr(\"A as ID\", \"B as Location\")\\nstore_info_df = spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"StoreInfo!A1:C10\").load().selectExpr(\"A as ID\", \"B as RegionID\", \"C as StoreOwner\")\\n\\n# Derive columns for SalesInfo using RegionInfo and StoreInfo\\nsales_info_df = sales_info_df.join(store_info_df, sales_info_df.StoreID == store_info_df.ID, \"left\") \\\\\\n                             .join(region_info_df, store_info_df.RegionID == region_info_df.ID, \"left\") \\\\\\n                             .withColumn(\"StoreLocation\", when(region_info_df.Location.isNotNull(), region_info_df.Location).otherwise(lit(\"\"))) \\\\\\n                             .withColumn(\"Owner\", when(store_info_df.StoreOwner.isNotNull(), store_info_df.StoreOwner).otherwise(lit(\"\"))) \\\\\\n                             .drop(store_info_df.ID, store_info_df.RegionID, store_info_df.StoreOwner, region_info_df.ID)\\n\\n# Derive columns for SaleGrowthReport1 using SalesInfo\\nsale_growth_report1_df = sales_info_df.groupBy(\"StoreID\") \\\\\\n                                      .agg(avg(col(\"SalesValue\")).alias(\"AverageSales\"), \\\\\\n                                           min(col(\"SalesValue\")).alias(\"MinSales\"), \\\\\\n                                           max(col(\"SalesValue\")).alias(\"MaxSales\"))\\n\\n# Derive columns for SaleGrowthReport2 using SalesInfo\\nsale_growth_report2_df = sales_info_df.withColumn(\"StoreLocation-SalesYear\", concat_ws(\"-\", col(\"StoreLocation\"), col(\"SalesYear\"))) \\\\\\n                                      .select(\"StoreLocation-SalesYear\", \"SalesYear\", \"StoreLocation\") \\\\\\n                                      .distinct() \\\\\\n                                      .withColumn(\"Year\", substring_index(col(\"StoreLocation-SalesYear\"), \"-\", -1)) \\\\\\n                                      .withColumn(\"Location\", substring_index(col(\"StoreLocation-SalesYear\"), \"-\", 1)) \\\\\\n                                      .alias(\"temp_df\") \\\\\\n                                      .join(sales_info_df.alias(\"si\"), (col(\"temp_df.Location\") == col(\"si.StoreLocation\")) & (col(\"temp_df.Year\") == col(\"si.SalesYear\")), \"inner\") \\\\\\n                                      .groupBy(\"temp_df.StoreLocation-SalesYear\", \"temp_df.Year\", \"temp_df.Location\") \\\\\\n                                      .agg(avg(col(\"si.SalesValue\")).alias(\"AverageSales\"), \\\\\\n                                           min(col(\"si.SalesValue\")).alias(\"MinSales\"), \\\\\\n                                           max(col(\"si.SalesValue\")).alias(\"MaxSales\"))\\n```\\n\\n### Notes:\\n- The `spark.read.format(\"excel\").option(\"header\", \"false\").option(\"dataAddress\", \"SheetName!A1:D10\").load()` is a placeholder for reading Excel sheets. The actual implementation may vary based on the availability of Excel readers like `pyspark-excel` or `spark-excel`.\\n- The actual column ranges (`A1:D10`) should be adjusted based on the actual data.\\n- The derived columns are created using appropriate PySpark functions and transformations.\\n- This code assumes that the Excel sheets are readable and properly formatted as per the provided dictionary.\\n\\nIf you need to make any modifications, feel free to specify the changes, and I will update the code accordingly.'}]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Initialize an empty list to maintain chat history & Message history\n",
    "chat_history = []\n",
    "msg_history = []\n",
    "inf_mdl_nm = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "file_uploaded = False  # Flag to track if a file is uploaded\n",
    "\n",
    "# Function to handle file upload\n",
    "def handle_file_upload(file):\n",
    "    global file_uploaded, chat_history, msg_history\n",
    "    if file:\n",
    "        # Reset histories on new file upload\n",
    "        chat_history = []\n",
    "        msg_history = []\n",
    "\n",
    "        file_uploaded = True  # Set the flag to True when a file is uploaded\n",
    "\n",
    "        # Extract the file name\n",
    "        file_name = file.name\n",
    "\n",
    "        initial_msg = gen_initial_msg(file_name)\n",
    "        cd_resp = generate_responses_from_inf(hf_token, initial_msg, inf_mdl_nm, 5000)\n",
    "\n",
    "        # Add the \"File Loaded\" message to msg_history only\n",
    "        msg_history.append(initial_msg[0])\n",
    "        msg_history.append(initial_msg[1])\n",
    "        \n",
    "        # Add the bot's response with the file name to both chat_history and msg_history\n",
    "        bot_message = f\"The file name is: {file_name}\"\n",
    "        chat_history.append((\"Bot\", \"\\n\"+cd_resp))\n",
    "        msg_history.append({'role': \"assistant\", 'content': cd_resp})\n",
    "\n",
    "        # Generate the formatted chat history\n",
    "        conversation_history = format_chat_history(chat_history)\n",
    "\n",
    "        return \"File uploaded successfully. You can now start chatting!\", conversation_history\n",
    "    else:\n",
    "        file_uploaded = False\n",
    "        return \"Please upload a valid Excel file.\", \"\"\n",
    "\n",
    "# Function to format chat history for display\n",
    "def format_chat_history(chat_history):\n",
    "    conversation_history = ''\n",
    "    for speaker, message in chat_history:\n",
    "        if speaker == \"User\":\n",
    "            conversation_history += f'**{speaker}:** {message}\\n\\n'\n",
    "        else:\n",
    "            conversation_history += f'**{speaker}:** {message}\\n\\n'\n",
    "    return conversation_history\n",
    "\n",
    "# Function to enable chat input and submit button after file upload\n",
    "def enable_chat_components(file_status):\n",
    "    if \"successfully\" in file_status.lower():\n",
    "        return gr.update(interactive=True), gr.update(interactive=True)\n",
    "    else:\n",
    "        return gr.update(interactive=False), gr.update(interactive=False)\n",
    "\n",
    "# Function to handle chat history\n",
    "def chatbot_history(user_input):\n",
    "    global chat_history  # Use the global chat_history list\n",
    "    global msg_history\n",
    "\n",
    "    # Append user input to the chat history\n",
    "    chat_history.append((\"User\", user_input))\n",
    "    msg_history.append({'role': \"User\", 'content': user_input})\n",
    "\n",
    "    # Generate a response (you can replace this with more complex logic)\n",
    "    cd_resp = generate_responses_from_inf(hf_token, msg_history, inf_mdl_nm, 5000)\n",
    "    msg_history.append({'role': \"assistant\", 'content': cd_resp})\n",
    "\n",
    "    # Append bot response to chat history\n",
    "    chat_history.append((\"Bot\",\"\\n\"+cd_resp))\n",
    "\n",
    "    # Create a formatted string of the conversation history\n",
    "    conversation_history = format_chat_history(chat_history)\n",
    "\n",
    "    print(msg_history)\n",
    "\n",
    "    # Return the formatted conversation history and clear the input field\n",
    "    return conversation_history, \"\"\n",
    "\n",
    "# Create Gradio interface using Blocks with Soft theme\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title with proper font\n",
    "    gr.HTML(\"\"\" \n",
    "        <div style=\"font-size: 2.5em; color: #4CAF50; font-weight: bold; text-align: center;\">\n",
    "            Oh Sheet!!!!! It's Spark\n",
    "        </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # Layout with chat and input section\n",
    "    with gr.Row():\n",
    "        # Message section on the left (30% width)\n",
    "        with gr.Column(scale=1, min_width=30):\n",
    "            # File uploader at the top\n",
    "            file_upload = gr.File(label=\"Upload an Excel File\", file_types=[\".xls\", \".xlsx\"], interactive=True)\n",
    "            file_status = gr.Textbox(label=\"\", interactive=False, lines=1, value=\"Please upload an Excel file to begin.\", show_label=False)\n",
    "\n",
    "            # Chatbox and submit button\n",
    "            input_text = gr.Textbox(label=\"Enter your message\", lines=1, interactive=False)  # Initially disabled\n",
    "            submit_button = gr.Button(\"Submit\", variant=\"primary\", interactive=False)  # Initially disabled\n",
    "\n",
    "        # Chatbot section on the right (70% width)\n",
    "        with gr.Column(scale=2, min_width=70):\n",
    "            output_text = gr.Markdown(label=\"Chatbot History\", elem_id=\"chat_history\")  # Use Markdown for rendering content\n",
    "\n",
    "    # Ensure file upload enables the chatbox\n",
    "    file_upload.change(fn=handle_file_upload, inputs=file_upload, outputs=[file_status, output_text])\n",
    "    file_status.change(fn=enable_chat_components, inputs=file_status, outputs=[input_text, submit_button])\n",
    "\n",
    "    # Ensure Enter key is mapped to submit functionality\n",
    "    input_text.submit(fn=chatbot_history, inputs=input_text, outputs=[output_text, input_text])\n",
    "    submit_button.click(fn=chatbot_history, inputs=input_text, outputs=[output_text, input_text])\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
