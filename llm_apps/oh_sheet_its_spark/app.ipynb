{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"{project_path}\\llm_custom_apps\\llm_apps\\oh_sheet_its_spark\\utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_history:  [{'role': 'system', 'content': \"\\nYou are a helpful assistant to a Data Scientist who is working on a project to transform Excel Spreadsheets to PySpark Dataframes. \\nThe Data Scientist has provided you with a dictionary of dictionaries which contains details about Excel spreadsheets.\\nEach dictionary has only one key which is the name of the Sheet and the value is a dictionary with multiple keys.\\nEach key is the name of the Column Header in the Excel Spreadsheet. The value is again a dictionary with two keys.\\nThe first key is 'ColumnID' with the value being the associated ColumnID in Excel i.e. A, B, C, or etc.\\nThe second key is 'ColumnValue' with the value being the associated formula for generating the column or the hardcoded value in the absence of the formula.\\nThe dictionary looks something like:\\n\\n{{'SheetNum1': {'X': {'ColumnID': 'A', 'ColumnValue': 11},\\n   'Y': {'ColumnID': 'B', 'ColumnValue': 2020},\\n   'Z': {'ColumnID': 'C', 'ColumnValue': 15789},\\n   'P': {'ColumnID': 'D', 'ColumnValue': 'KIO'},\\n   'Q': {'ColumnID': 'E', 'ColumnValue': 'SECCC'}}},\\n {'SheetNum2': {'ID': {'ColumnID': 'A', 'LLOP': 1},\\n   'Location': {'ColumnID': 'B', 'ColumnValue': 'LOPP'}}}}\\n\\nYou have to convert the spreadsheet transformation logic to pyspark dataframe keeping the following in mind\\n\\n1) Parsing the dictionary figure out the dependency between sheets and between columns to identify the order in which the transformations need to be defined and the columns that are hardcoded.\\n2) In your response at the beginning provide explanation about the Sheets that are present.For each Sheet specify what columns are hardcoded and what columns are derived .Do not specify the values of the columns but only the column names.\\n3) Now respond with code with the code block starting with ```python and ending with ``` . Ensure:\\n  a) All the python dependencies that will be required to accomplish this task are imported and after declaring dependencies, also create a placeholder to read the spreadsheet creating multiple pyspark dataframes , one for each sheet in the spreadsheet if and only if it has atleast one hardcoded column (i.e. not derived using formula) and only read/select the hardcoded columns from dataframe.Rememeber I want you to create Dataframes from the individual sheets of spreadsheet and not creating them explictly from the hardcoded values in the dictionary provided\\n  b) Now create new dataframes from the dataframes declared as input and derive transformed columns for each of the column that is derived using formulas based on dictionary provided , with column name in pyspark being same as Column Header provided in the dictionary.\\n  c) Always have in comments the Psuedo Code(Logic in plain english used to derive the column,followed by excel formula.) on the same line against each transformation (withColumn,agg methods, join etc. ) .\\n  d) Subsequently the user might ask for modications for removing /adding /modifying existing transformations to which you should comply accordingly by updating entire code but keep explanation short and bare minimum.\\n\\n\\n\\n\"}, {'role': 'user', 'content': 'Convert the excel logic representation to equivalent pyspark code : {\\'SalesInfo\\': {\\'StoreID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'11\\'}, \\'SalesYear\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'2020\\'}, \\'SalesValue\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'15789\\'}, \\'StoreLocation\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\")\\'}, \\'Owner\\': {\\'ColumnID\\': \\'E\\', \\'ColumnValue\\': \\'=IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\")\\'}}, \\'RegionInfo\\': {\\'ID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'1\\'}, \\'Location\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'Halifax\\'}}, \\'StoreInfo\\': {\\'ID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'11\\'}, \\'RegionID\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'1\\'}, \\'StoreOwner\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'Ethan Carter\\'}}, \\'SaleGrowthReport1\\': {\\'StoreID\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'=_xlfn.UNIQUE(_xlfn._xlws.FILTER(SalesInfo!A:A, SalesInfo!A:A<>\"\"))\\'}, \\'AverageSales\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'=AVERAGEIF(SalesInfo!A:A, A2, SalesInfo!C:C)\\'}, \\'MinSales\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'=MIN(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\'}, \\'MaxSales\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=MAX(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\'}}, \\'SaleGrowthReport2\\': {\\'StoreLocation-SalesYear\\': {\\'ColumnID\\': \\'A\\', \\'ColumnValue\\': \\'=_xlfn.UNIQUE(SalesInfo!D:D & \"-\" & SalesInfo!B:B)\\'}, \\'Year\\': {\\'ColumnID\\': \\'B\\', \\'ColumnValue\\': \\'=RIGHT(A2, LEN(A2) - FIND(\"-\", A2))\\'}, \\'Location\\': {\\'ColumnID\\': \\'C\\', \\'ColumnValue\\': \\'=LEFT(A2, FIND(\"-\", A2) - 1)\\'}, \\'AverageSales\\': {\\'ColumnID\\': \\'D\\', \\'ColumnValue\\': \\'=AVERAGEIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}, \\'MinSales\\': {\\'ColumnID\\': \\'E\\', \\'ColumnValue\\': \\'=_xlfn.MINIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}, \\'MaxSales\\': {\\'ColumnID\\': \\'F\\', \\'ColumnValue\\': \\'=_xlfn.MAXIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\'}}}'}, {'role': 'assistant', 'content': '### Explanation\\n\\n#### Sheets and Columns Explanation\\n\\n**SalesInfo:**\\n- **Hardcoded Columns:** StoreID, SalesYear, SalesValue\\n- **Derived Columns:** StoreLocation, Owner\\n\\n**RegionInfo:**\\n- **Hardcoded Columns:** ID, Location\\n- **Derived Columns:** None\\n\\n**StoreInfo:**\\n- **Hardcoded Columns:** ID, RegionID, StoreOwner\\n- **Derived Columns:** None\\n\\n**SaleGrowthReport1:**\\n- **Hardcoded Columns:** None\\n- **Derived Columns:** StoreID, AverageSales, MinSales, MaxSales\\n\\n**SaleGrowthReport2:**\\n- **Hardcoded Columns:** None\\n- **Derived Columns:** StoreLocation-SalesYear, Year, Location, AverageSales, MinSales, MaxSales\\n\\n\\n### Pyspark Code\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, avg, min, max, when, isnull, first, expr, split, regexp_extract, lpad, concat_ws\\n\\n# Create Spark Session\\nspark = SparkSession.builder \\\\\\n    .appName(\"ExcelToPySpark\") \\\\\\n    .getOrCreate()\\n\\n# Placeholder to read the excel sheets creating multiple pyspark dataframes, one for each sheet\\n# Only hardcoded columns are read and selected.\\n\\n# Reading Sheets with hardcoded columns\\nsales_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"SalesInfo\").selectExpr(\"A as StoreID\", \"B as SalesYear\", \"C as SalesValue\")\\nregion_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"RegionInfo\").selectExpr(\"A as ID\", \"B as Location\")\\nstore_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"StoreInfo\").selectExpr(\"A as ID\", \"B as RegionID\", \"C as StoreOwner\")\\n\\n# Transformations for SalesInfo dataframe\\nsales_info_df = sales_info_df.join(store_info_df.withColumnRenamed(\"ID\", \"StoreID\"), on=\"StoreID\", how=\"left\") \\\\\\n    .withColumn(\"Owner\", when(isnull(col(\"StoreOwner\")), \"\").otherwise(col(\"StoreOwner\"))) \\\\\\n    .drop(\"StoreOwner\")  # Pseudo Code: IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\") | =IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\")\\n\\nsales_info_df = sales_info_df.join(region_info_df.withColumnRenamed(\"ID\", \"RegionID\"), on=\"RegionID\", how=\"left\") \\\\\\n    .withColumn(\"StoreLocation\", when(isnull(col(\"Location\")), \"\").otherwise(col(\"Location\"))) \\\\\\n    .drop(\"RegionID\", \"Location\")  # Pseudo Code: IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\") | =IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\")\\n\\n# Transformation for SaleGrowthReport1 dataframe\\nsale_growth_report1_df = sales_info_df.groupBy(\"StoreID\").agg(\\n    avg(\"SalesValue\").alias(\"AverageSales\"),  # Pseudo Code: AVERAGEIF(SalesInfo!A:A, A2, SalesInfo!C:C)\\n    min(\"SalesValue\").alias(\"MinSales\"),      # Pseudo Code: MIN(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\n    max(\"SalesValue\").alias(\"MaxSales\")       # Pseudo Code: MAX(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\n).distinct()  # Pseudo Code: _xlfn.UNIQUE(_xlfn._xlws.FILTER(SalesInfo!A:A, SalesInfo!A:A<>\"\"))\\n\\n# Transformation for SaleGrowthReport2 dataframe\\nsale_growth_report2_df = sales_info_df.selectExpr(\"concat_ws(\\'-\\', StoreLocation, SalesYear) as `StoreLocation-SalesYear`\", \"StoreLocation\", \"SalesYear\", \"SalesValue\")\\n\\nsale_growth_report2_df = sale_growth_report2_df.groupBy(\"StoreLocation-SalesYear\").agg(\\n    first(regexp_extract(col(\"StoreLocation-SalesYear\"), r\\'^(.*)-.+$\\', 1)).alias(\"Location\"),  # Pseudo Code: LEFT(A2, FIND(\"-\", A2) - 1)\\n    first(regexp_extract(col(\"StoreLocation-SalesYear\"), r\\'^.+-([0-9]+)$\\', 1)).alias(\"Year\"),    # Pseudo Code: RIGHT(A2, LEN(A2) - FIND(\"-\", A2))\\n    avg(\"SalesValue\").alias(\"AverageSales\"),                                                # Pseudo Code: AVERAGEIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n    min(\"SalesValue\").alias(\"MinSales\"),                                                    # Pseudo Code: _xlfn.MINIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n    max(\"SalesValue\").alias(\"MaxSales\")                                                     # Pseudo Code: _xlfn.MAXIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n).distinct()\\n```\\n\\nThis code snippet reads the necessary columns from the Excel sheets and applies the required transformations to generate new dataframes based on the provided dictionary of dictionaries. The transformations are done using Pyspark\\'s DataFrame API, with comments explaining the logic used in Excel formulas.'}, {'role': 'user', 'content': \"Can you add a column called VS2 in SalesGrowthReport2 dataframe and hardcode it to 'PCO'\"}, {'role': 'assistant', 'content': 'Certainly! I\\'ll add a new column called `VS2` and hardcode it to `\\'PCO\\'` in the `SaleGrowthReport2` dataframe. Here is the updated code:\\n\\n### Explanation\\n\\n#### Sheets and Columns Explanation\\n\\n**SalesInfo:**\\n- **Hardcoded Columns:** StoreID, SalesYear, SalesValue\\n- **Derived Columns:** StoreLocation, Owner\\n\\n**RegionInfo:**\\n- **Hardcoded Columns:** ID, Location\\n- **Derived Columns:** None\\n\\n**StoreInfo:**\\n- **Hardcoded Columns:** ID, RegionID, StoreOwner\\n- **Derived Columns:** None\\n\\n**SaleGrowthReport1:**\\n- **Hardcoded Columns:** None\\n- **Derived Columns:** StoreID, AverageSales, MinSales, MaxSales\\n\\n**SaleGrowthReport2:**\\n- **Hardcoded Columns:** VS2 (hardcoded to \\'PCO\\')\\n- **Derived Columns:** StoreLocation-SalesYear, Year, Location, AverageSales, MinSales, MaxSales\\n\\n### Pyspark Code\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, avg, min, max, when, isnull, first, expr, split, regexp_extract, concat_ws, lit\\n\\n# Create Spark Session\\nspark = SparkSession.builder \\\\\\n    .appName(\"ExcelToPySpark\") \\\\\\n    .getOrCreate()\\n\\n# Placeholder to read the excel sheets creating multiple pyspark dataframes, one for each sheet\\n# Only hardcoded columns are read and selected.\\n\\n# Reading Sheets with hardcoded columns\\nsales_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"SalesInfo\").selectExpr(\"A as StoreID\", \"B as SalesYear\", \"C as SalesValue\")\\nregion_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"RegionInfo\").selectExpr(\"A as ID\", \"B as Location\")\\nstore_info_df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").excel(\"path_to_excel_file.xlsx\", sheetName=\"StoreInfo\").selectExpr(\"A as ID\", \"B as RegionID\", \"C as StoreOwner\")\\n\\n# Transformations for SalesInfo dataframe\\nsales_info_df = sales_info_df.join(store_info_df.withColumnRenamed(\"ID\", \"StoreID\"), on=\"StoreID\", how=\"left\") \\\\\\n    .withColumn(\"Owner\", when(isnull(col(\"StoreOwner\")), \"\").otherwise(col(\"StoreOwner\"))) \\\\\\n    .drop(\"StoreOwner\")  # Pseudo Code: IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\") | =IFERROR(INDEX(StoreInfo!C:C, MATCH(A2, StoreInfo!A:A, 0)), \"\")\\n\\nsales_info_df = sales_info_df.join(region_info_df.withColumnRenamed(\"ID\", \"RegionID\"), on=\"RegionID\", how=\"left\") \\\\\\n    .withColumn(\"StoreLocation\", when(isnull(col(\"Location\")), \"\").otherwise(col(\"Location\"))) \\\\\\n    .drop(\"RegionID\", \"Location\")  # Pseudo Code: IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\") | =IFERROR(INDEX(RegionInfo!B:B, MATCH(INDEX(StoreInfo!B:B, MATCH(A2, StoreInfo!A:A, 0)), RegionInfo!A:A, 0)), \"\")\\n\\n# Transformation for SaleGrowthReport1 dataframe\\nsale_growth_report1_df = sales_info_df.groupBy(\"StoreID\").agg(\\n    avg(\"SalesValue\").alias(\"AverageSales\"),  # Pseudo Code: AVERAGEIF(SalesInfo!A:A, A2, SalesInfo!C:C)\\n    min(\"SalesValue\").alias(\"MinSales\"),      # Pseudo Code: MIN(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\n    max(\"SalesValue\").alias(\"MaxSales\")       # Pseudo Code: MAX(IF(SalesInfo!A:A=A2, SalesInfo!C:C))\\n).distinct()  # Pseudo Code: _xlfn.UNIQUE(_xlfn._xlws.FILTER(SalesInfo!A:A, SalesInfo!A:A<>\"\"))\\n\\n# Transformation for SaleGrowthReport2 dataframe\\nsale_growth_report2_df = sales_info_df.selectExpr(\"concat_ws(\\'-\\', StoreLocation, SalesYear) as `StoreLocation-SalesYear`\", \"StoreLocation\", \"SalesYear\", \"SalesValue\")\\n\\nsale_growth_report2_df = sale_growth_report2_df.groupBy(\"StoreLocation-SalesYear\").agg(\\n    first(regexp_extract(col(\"StoreLocation-SalesYear\"), r\\'^(.*)-.+$\\', 1)).alias(\"Location\"),  # Pseudo Code: LEFT(A2, FIND(\"-\", A2) - 1)\\n    first(regexp_extract(col(\"StoreLocation-SalesYear\"), r\\'^.+-([0-9]+)$\\', 1)).alias(\"Year\"),    # Pseudo Code: RIGHT(A2, LEN(A2) - FIND(\"-\", A2))\\n    avg(\"SalesValue\").alias(\"AverageSales\"),                                                # Pseudo Code: AVERAGEIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n    min(\"SalesValue\").alias(\"MinSales\"),                                                    # Pseudo Code: _xlfn.MINIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n    max(\"SalesValue\").alias(\"MaxSales\")                                                     # Pseudo Code: _xlfn.MAXIFS(SalesInfo!C:C, SalesInfo!B:B, B2, SalesInfo!D:D, C2)\\n).distinct()\\n\\n# Add hardcoded column VS2 in SaleGrowthReport2 dataframe\\nsale_growth_report2_df = sale_growth_report2_df.withColumn(\"VS2\", lit(\"PCO\"))  # Pseudo Code: Hardcoded to \\'PCO\\'\\n```\\n\\nIn this updated code, the `VS2` column is added to the `sale_growth_report2_df` dataframe using the `lit` function to hardcode the value `\\'PCO\\'`.'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to maintain chat history & message history\n",
    "conversation_history = []\n",
    "msg_history = []\n",
    "formatted_history = \"\"  # Variable to store the formatted chat history\n",
    "inf_mdl_nm = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "file_uploaded = False  # Flag to track if a file is uploaded\n",
    "\n",
    "# Function to handle file upload\n",
    "def handle_file_upload(file):\n",
    "    global file_uploaded, conversation_history, msg_history, formatted_history\n",
    "    if file:\n",
    "        # Reset histories on new file upload\n",
    "        conversation_history = []\n",
    "        msg_history = []\n",
    "        formatted_history = \"\"  # Reset formatted history as well\n",
    "\n",
    "        file_uploaded = True  # Set the flag to True when a file is uploaded\n",
    "\n",
    "        # Extract the file name\n",
    "        file_name = file.name\n",
    "\n",
    "        initial_msg = gen_initial_msg(file_name)\n",
    "        cd_resp = generate_responses_from_inf(hf_token, initial_msg, inf_mdl_nm, 5000)\n",
    "\n",
    "        # Add the \"File Loaded\" message to msg_history only\n",
    "        msg_history.append(initial_msg[0])\n",
    "        msg_history.append(initial_msg[1])\n",
    "        \n",
    "        # Add the bot's response with the file name to both conversation_history and msg_history\n",
    "        conversation_history.append((\"Bot\", \"\\n\\n\" + cd_resp))\n",
    "        msg_history.append({'role': \"assistant\", 'content': cd_resp})\n",
    "\n",
    "        # Format the initial bot message and append to formatted history\n",
    "        formatted_history = format_message(\"Bot\", \"\\n\\n\" + cd_resp)\n",
    "\n",
    "        # Generate the formatted chat history for display\n",
    "        return \"File uploaded successfully. You can now start chatting!\", formatted_history\n",
    "    else:\n",
    "        file_uploaded = False\n",
    "        return \"Please upload a valid Excel file.\", \"\"\n",
    "\n",
    "# Function to format each message with proper styling\n",
    "def format_message(speaker, message):\n",
    "    if speaker == \"User\":\n",
    "        return f'<div style=\"background-color: #FFEBEE; padding: 10px; border-radius: 10px; margin-bottom: 10px; text-align: right; color: #6a1b9a;\"><strong>{speaker}:</strong> {message}</div>'\n",
    "    else:\n",
    "        # Bot messages now have the word \"Bot:\" styled in #cd7337 color\n",
    "        return f'<div style=\"background-color: #e0f7fa; padding: 10px; border-radius: 10px; margin-bottom: 10px; text-align: left; color: #000;\"><strong style=\"color: #cd7337;\">{speaker}:</strong> {message}</div>'\n",
    "\n",
    "# Function to handle chat history and generate responses\n",
    "def chatbot_history(user_input):\n",
    "    global conversation_history  # Use the global conversation_history list\n",
    "    global msg_history\n",
    "    global formatted_history  # Use the global formatted_history\n",
    "\n",
    "    # Append user input to the conversation history\n",
    "    conversation_history.append((\"User\", user_input))\n",
    "    msg_history.append({'role': \"user\", 'content': str(user_input)})\n",
    "\n",
    "    # Generate a response (for testing, reversing user input)\n",
    "    bot_response = generate_responses_from_inf(hf_token, msg_history, inf_mdl_nm, 5000)\n",
    "    msg_history.append({'role': \"assistant\", 'content': bot_response})\n",
    "\n",
    "    print(\"msg_history: \", msg_history)\n",
    "\n",
    "    # Append bot response to conversation history\n",
    "    conversation_history.append((\"Bot\", bot_response))\n",
    "\n",
    "    # Format the current user and bot message\n",
    "    current_conversation = format_message(\"User\", user_input) + format_message(\"Bot\", \"\\n\\n\" +bot_response)\n",
    "\n",
    "    # Combine previous formatted history with the new formatted message\n",
    "    formatted_history += current_conversation\n",
    "\n",
    "    # Return the updated formatted history\n",
    "    return gr.update(value=formatted_history), \"\"  # Return the current formatted history\n",
    "\n",
    "# Function to enable chat input and submit button after file upload\n",
    "def enable_chat_components(file_status):\n",
    "    if \"successfully\" in file_status.lower():\n",
    "        return gr.update(interactive=True), gr.update(interactive=True)\n",
    "    else:\n",
    "        return gr.update(interactive=False), gr.update(interactive=False)\n",
    "\n",
    "# Create Gradio interface using Blocks with Soft theme\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title with proper font\n",
    "    gr.HTML(\"\"\" \n",
    "        <div style=\"font-size: 2.5em; color: #4CAF50; font-weight: bold; text-align: center;\">\n",
    "            Oh Sheet!!!!! It's Spark\n",
    "        </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # Layout with chat and input section\n",
    "    with gr.Row():\n",
    "        # Message section on the left (30% width)\n",
    "        with gr.Column(scale=1, min_width=30):\n",
    "            # File uploader at the top\n",
    "            file_upload = gr.File(label=\"Upload an Excel File\", file_types=[\".xls\", \".xlsx\"], interactive=True)\n",
    "            file_status = gr.Textbox(label=\"\", interactive=False, lines=1, value=\"Please upload an Excel file to begin.\", show_label=False)\n",
    "\n",
    "            # Chatbox and submit button\n",
    "            input_text = gr.Textbox(label=\"Enter your message\", lines=1, interactive=False)  # Initially disabled\n",
    "            submit_button = gr.Button(\"Submit\", variant=\"primary\", interactive=False)  # Initially disabled\n",
    "\n",
    "        # Chatbot section on the right (70% width)\n",
    "        with gr.Column(scale=2, min_width=70):\n",
    "            output_text = gr.Markdown(label=\"Chatbot History\", elem_id=\"chat_history\")  # Use Markdown for rendering content\n",
    "\n",
    "    # Ensure file upload enables the chatbox\n",
    "    file_upload.change(fn=handle_file_upload, inputs=file_upload, outputs=[file_status, output_text])\n",
    "    file_status.change(fn=enable_chat_components, inputs=file_status, outputs=[input_text, submit_button])\n",
    "\n",
    "    # Ensure Enter key is mapped to submit functionality\n",
    "    input_text.submit(fn=chatbot_history, inputs=input_text, outputs=[output_text, input_text])\n",
    "    submit_button.click(fn=chatbot_history, inputs=input_text, outputs=[output_text, input_text])\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
