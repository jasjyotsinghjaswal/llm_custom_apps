{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import json\n",
    "import importlib\n",
    "import sys\n",
    "import configparser\n",
    "import requests\n",
    "import os\n",
    "import openpyxl\n",
    "import json\n",
    "import xlwings as xw\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig,TextStreamer,pipeline\n",
    "from huggingface_hub import InferenceApi,InferenceClient\n",
    "import time\n",
    "from io import StringIO\n",
    "from IPython.display import display, HTML,Markdown\n",
    "import gradio as gr\n",
    "#from pyspark import SparkConf\n",
    "#from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Env Variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables from .env\n",
    "load_dotenv()\n",
    "os.environ[\"SPARK_HOME\"] = os.getenv(\"PROJ_SPARK_HOME\",\"\")\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\",\"\")\n",
    "hf_token=os.getenv(\"HF_TOKEN\",\"\")\n",
    "project_path = os.getenv(\"PROJECT_PATH\")\n",
    "dataset_path = f\"{project_path}\\llm_custom_apps\\\\datasets\"\n",
    "test_data_path = f\"{dataset_path}\\\\test_datasets\"\n",
    "app_data_path = f\"{dataset_path}\\\\app_datasets\"\n",
    "quant_model_path = f\"{project_path}\\\\quant_models\"\n",
    "# Set the custom cache directory for Hugging Face Transformers\n",
    "os.environ['HF_HOME'] = f\"{project_path}\\llm_custom_apps\\\\.hf_cache_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(logger_nm):\n",
    "    \"\"\"\n",
    "    Function takes a Logger Name and returns a Generic Logger\n",
    "    :param logger_nm: Name of the Logger that is prefixed to the Log Statements\n",
    "    :return: logger object to be used across different scripts\n",
    "    \"\"\"\n",
    "\n",
    "    # Define generic logger variable\n",
    "    gen_logger = logging.getLogger(logger_nm)\n",
    "    # a) Create Streaming Handler and Set level to Debug\n",
    "    gen_logger.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    # b) Create formatter\n",
    "    lg_str = \"\\n%(name)s - %(levelname)s - %(lineno)s - %(funcName)s - %(asctime)s - %(message)s\"\n",
    "    formatter = logging.Formatter(lg_str)\n",
    "    # c) Add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "    # d) Add ch to logger after clearing previous handlers\n",
    "    gen_logger.handlers.clear()\n",
    "    gen_logger.addHandler(ch)\n",
    "\n",
    "    return gen_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_logger = create_logger(\"Generic Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Rest API method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reusable_rest_api(api_url, req_data=None, cert_verify=False, header=None, method=\"GET\", resp=\"JSON\", retry_lim=3):\n",
    "    \"\"\"\n",
    "    Make HTTP requests to a specified API endpoint with various options.\n",
    "\n",
    "    Parameters:\n",
    "    - api_url (str): The URL of the API endpoint to make the request to.\n",
    "    - req_data (str or dict, optional): The request data to send to the API (e.g., JSON payload). Default is None.\n",
    "    - cert_verify (bool, optional): Whether to verify the SSL certificate. Default is False.\n",
    "    - header (dict, optional): Additional headers to include in the request. Default is None.\n",
    "    - method (str, optional): The HTTP request method to use (GET, POST, PUT, DELETE). Default is \"GET\".\n",
    "    - resp (str, optional): The expected response type (\"JSON\" or \"RAW\"). Default is \"JSON\".\n",
    "    - retry_lim (int, optional): The maximum number of retry attempts in case of connection errors. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the HTTP status code of the response and the response content.\n",
    "\n",
    "    Raises:\n",
    "    - requests.exceptions.HTTPError: If an invalid API method is specified.\n",
    "    - requests.exceptions.ConnectionError: If a connection error occurs, and the maximum retry limit is exceeded.\n",
    "    - requests.exceptions.Timeout: If a timeout occurs while making the request.\n",
    "    - requests.exceptions.RequestException: If a general request exception occurs.\n",
    "    - ValueError: If there is an issue parsing the response content as JSON.\n",
    "\n",
    "    Example usage:\n",
    "    status_code, response_data = reusable_rest_api(\n",
    "        api_url=\"https://example.com/api\",\n",
    "        req_data={\"key\": \"value\"},\n",
    "        cert_verify=True,\n",
    "        header={\"Authorization\": \"Bearer Token\"},\n",
    "        method=\"POST\",\n",
    "        resp=\"JSON\",\n",
    "        retry_lim=3\n",
    "    )\n",
    "    \"\"\"\n",
    "    retry = 1\n",
    "    while True:\n",
    "        resp_content = \"\"\n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                resp_content = requests.get(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"POST\":\n",
    "                resp_content = requests.post(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"PUT\":\n",
    "                resp_content = requests.put(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"DELETE\":\n",
    "                resp_content = requests.delete(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                resp_json = None\n",
    "            else:\n",
    "                raise requests.exceptions.HTTPError(\"Invalid API Method\")\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.ConnectionError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            retry = retry + 1\n",
    "            if retry <= retry_lim+1:  continue\n",
    "            gen_logger.error(\"Retry Limit Exceeded\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.Timeout as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except ValueError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            retry = retry + 1\n",
    "            if retry <= retry_lim+1:  continue\n",
    "            gen_logger.error(\"Retry Limit Exceeded\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        final_resp = resp_json if (resp == \"JSON\") else resp_content\n",
    "        return resp_content.status_code,final_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Saving Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model_with_bitsandbytes(model_name: str, quant_model_directory: str, quantization_bits: int = 4):\n",
    "    \"\"\"\n",
    "    Quantize a Hugging Face model using bitsandbytes for 4-bit quantization.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the Hugging Face model to quantize.\n",
    "        quant_model_directory (str): Directory to save the quantized model.\n",
    "        quantization_bits (int): Number of bits for quantization (default is 4).\n",
    "    \"\"\"\n",
    "    output_dir = f\"{quant_model_path}/{quant_model_directory}\"\n",
    "    # Validate input\n",
    "    if quantization_bits != 4:\n",
    "        raise ValueError(\"Only 4-bit quantization is supported using bitsandbytes.\")\n",
    "    \n",
    "    # Set up quantization configuration\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    print(\"Loading model with quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Quantized model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Loading Response from Quantised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model_name: str, messages: list):\n",
    "    \"\"\"\n",
    "    Generate a response using a conversational model from Hugging Face, stream the response in real-time, \n",
    "    and clean up resources after execution.\n",
    "\n",
    "    This function loads a pre-trained language model and tokenizer from Hugging Face, tokenizes the input \n",
    "    messages, and streams the generated response token-by-token. After the response is generated (or if an \n",
    "    error occurs), the resources such as model, tokenizer, inputs, and streamer are cleaned up manually to \n",
    "    free GPU memory.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name or path of the pre-trained model to use from Hugging Face.\n",
    "        messages (list): A list of messages to provide as input to the model. Each message is expected to \n",
    "                         follow the format required for conversational models (e.g., a list of alternating \n",
    "                         user and assistant messages).\n",
    "                         \n",
    "    Returns:\n",
    "        None: The function prints the generated response in real-time and clears resources upon completion.\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    streamer = None\n",
    "    inputs = None\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer from Hugging Face\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Convert messages to model input format\n",
    "        print(\"Tokenizing input messages...\")\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Set up the TextStreamer to stream tokens as they are generated\n",
    "        print(\"Setting up streamer for real-time output...\")\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"Generating response...\")\n",
    "        model.generate(inputs['input_ids'], max_new_tokens=50, streamer=streamer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up resources manually if the function fails\n",
    "        if model is not None:\n",
    "            del model  # Delete the model\n",
    "            print(\"Model deleted\")\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            del tokenizer  # Delete the tokenizer\n",
    "            print(\"Tokenizer deleted\")\n",
    "\n",
    "        if inputs is not None:\n",
    "            del inputs  # Delete the inputs\n",
    "            print(\"Inputs deleted\")\n",
    "\n",
    "        if streamer is not None:\n",
    "            del streamer  # Delete the streamer\n",
    "            print(\"Streamer deleted\")\n",
    "\n",
    "        # Clear GPU memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_from_inf(hf_token, messages, model_name, max_tokens):\n",
    "    \"\"\"\n",
    "    Generate an HTML response by sending a chat completion request to the Hugging Face API.\n",
    "\n",
    "    Parameters:\n",
    "        hf_token (str): The Hugging Face API token.\n",
    "        messages (list): The messages to be sent in the chat completion request.\n",
    "        model_name (str): The name of the model to use for the chat completion.\n",
    "        max_tokens (int): The maximum number of tokens for the completion.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the model's response, or an error message if the request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Inference Client\n",
    "        client = InferenceClient(api_key=hf_token)\n",
    "\n",
    "        # Request a chat completion\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            messages=messages, \n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        response_content = completion.choices[0].message[\"content\"]\n",
    "\n",
    "        # Return the content\n",
    "        return response_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return error message if there is an exception\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return f\"Model ({model_name}) Busy or Unavailable. Try again after sometime\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
