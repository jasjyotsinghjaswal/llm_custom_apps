{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import json\n",
    "import importlib\n",
    "import sys\n",
    "import configparser\n",
    "import requests\n",
    "import os\n",
    "import openpyxl\n",
    "import json\n",
    "import xlwings as xw\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig,TextStreamer,pipeline\n",
    "from huggingface_hub import InferenceApi,InferenceClient\n",
    "import time\n",
    "from io import StringIO\n",
    "import markdown\n",
    "from IPython.display import display, HTML\n",
    "#from pyspark import SparkConf\n",
    "#from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install torch transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Env Variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables from .env\n",
    "load_dotenv()\n",
    "os.environ[\"SPARK_HOME\"] = os.getenv(\"PROJ_SPARK_HOME\",\"\")\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\",\"\")\n",
    "hf_token=os.getenv(\"HF_TOKEN\",\"\")\n",
    "project_path = os.getenv(\"PROJECT_PATH\")\n",
    "dataset_path = f\"{project_path}\\llm_custom_apps\\\\datasets\"\n",
    "test_data_path = f\"{dataset_path}\\\\test_datasets\"\n",
    "app_data_path = f\"{dataset_path}\\\\app_datasets\"\n",
    "quant_model_path = f\"{project_path}\\\\quant_models\"\n",
    "# Set the custom cache directory for Hugging Face Transformers\n",
    "os.environ['HF_HOME'] = f\"{project_path}\\llm_custom_apps\\\\.hf_cache_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(logger_nm):\n",
    "    \"\"\"\n",
    "    Function takes a Logger Name and returns a Generic Logger\n",
    "    :param logger_nm: Name of the Logger that is prefixed to the Log Statements\n",
    "    :return: logger object to be used across different scripts\n",
    "    \"\"\"\n",
    "\n",
    "    # Define generic logger variable\n",
    "    gen_logger = logging.getLogger(logger_nm)\n",
    "    # a) Create Streaming Handler and Set level to Debug\n",
    "    gen_logger.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    # b) Create formatter\n",
    "    lg_str = \"\\n%(name)s - %(levelname)s - %(lineno)s - %(funcName)s - %(asctime)s - %(message)s\"\n",
    "    formatter = logging.Formatter(lg_str)\n",
    "    # c) Add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "    # d) Add ch to logger after clearing previous handlers\n",
    "    gen_logger.handlers.clear()\n",
    "    gen_logger.addHandler(ch)\n",
    "\n",
    "    return gen_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_logger = create_logger(\"Generic Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Rest API method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reusable_rest_api(api_url, req_data=None, cert_verify=False, header=None, method=\"GET\", resp=\"JSON\", retry_lim=3):\n",
    "    \"\"\"\n",
    "    Make HTTP requests to a specified API endpoint with various options.\n",
    "\n",
    "    Parameters:\n",
    "    - api_url (str): The URL of the API endpoint to make the request to.\n",
    "    - req_data (str or dict, optional): The request data to send to the API (e.g., JSON payload). Default is None.\n",
    "    - cert_verify (bool, optional): Whether to verify the SSL certificate. Default is False.\n",
    "    - header (dict, optional): Additional headers to include in the request. Default is None.\n",
    "    - method (str, optional): The HTTP request method to use (GET, POST, PUT, DELETE). Default is \"GET\".\n",
    "    - resp (str, optional): The expected response type (\"JSON\" or \"RAW\"). Default is \"JSON\".\n",
    "    - retry_lim (int, optional): The maximum number of retry attempts in case of connection errors. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the HTTP status code of the response and the response content.\n",
    "\n",
    "    Raises:\n",
    "    - requests.exceptions.HTTPError: If an invalid API method is specified.\n",
    "    - requests.exceptions.ConnectionError: If a connection error occurs, and the maximum retry limit is exceeded.\n",
    "    - requests.exceptions.Timeout: If a timeout occurs while making the request.\n",
    "    - requests.exceptions.RequestException: If a general request exception occurs.\n",
    "    - ValueError: If there is an issue parsing the response content as JSON.\n",
    "\n",
    "    Example usage:\n",
    "    status_code, response_data = reusable_rest_api(\n",
    "        api_url=\"https://example.com/api\",\n",
    "        req_data={\"key\": \"value\"},\n",
    "        cert_verify=True,\n",
    "        header={\"Authorization\": \"Bearer Token\"},\n",
    "        method=\"POST\",\n",
    "        resp=\"JSON\",\n",
    "        retry_lim=3\n",
    "    )\n",
    "    \"\"\"\n",
    "    retry = 1\n",
    "    while True:\n",
    "        resp_content = \"\"\n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                resp_content = requests.get(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"POST\":\n",
    "                resp_content = requests.post(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"PUT\":\n",
    "                resp_content = requests.put(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                if resp == \"JSON\": resp_json = json.loads(resp_content.content)\n",
    "            elif method == \"DELETE\":\n",
    "                resp_content = requests.delete(api_url, header=header, data=req_data, verify=cert_verify)\n",
    "                resp_json = None\n",
    "            else:\n",
    "                raise requests.exceptions.HTTPError(\"Invalid API Method\")\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.ConnectionError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            retry = retry + 1\n",
    "            if retry <= retry_lim+1:  continue\n",
    "            gen_logger.error(\"Retry Limit Exceeded\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.Timeout as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            sys.exit(1)\n",
    "        except ValueError as err:\n",
    "            gen_logger.error(f\"The exception while calling this api is: {err}\")\n",
    "            retry = retry + 1\n",
    "            if retry <= retry_lim+1:  continue\n",
    "            gen_logger.error(\"Retry Limit Exceeded\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        final_resp = resp_json if (resp == \"JSON\") else resp_content\n",
    "        return resp_content.status_code,final_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Saving Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model_with_bitsandbytes(model_name: str, quant_model_directory: str, quantization_bits: int = 4):\n",
    "    \"\"\"\n",
    "    Quantize a Hugging Face model using bitsandbytes for 4-bit quantization.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the Hugging Face model to quantize.\n",
    "        quant_model_directory (str): Directory to save the quantized model.\n",
    "        quantization_bits (int): Number of bits for quantization (default is 4).\n",
    "    \"\"\"\n",
    "    output_dir = f\"{quant_model_path}/{quant_model_directory}\"\n",
    "    # Validate input\n",
    "    if quantization_bits != 4:\n",
    "        raise ValueError(\"Only 4-bit quantization is supported using bitsandbytes.\")\n",
    "    \n",
    "    # Set up quantization configuration\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    print(\"Loading model with quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Quantized model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Loading Response from Quantised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model_name: str, messages: list):\n",
    "    \"\"\"\n",
    "    Generate a response using a conversational model from Hugging Face, stream the response in real-time, \n",
    "    and clean up resources after execution.\n",
    "\n",
    "    This function loads a pre-trained language model and tokenizer from Hugging Face, tokenizes the input \n",
    "    messages, and streams the generated response token-by-token. After the response is generated (or if an \n",
    "    error occurs), the resources such as model, tokenizer, inputs, and streamer are cleaned up manually to \n",
    "    free GPU memory.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name or path of the pre-trained model to use from Hugging Face.\n",
    "        messages (list): A list of messages to provide as input to the model. Each message is expected to \n",
    "                         follow the format required for conversational models (e.g., a list of alternating \n",
    "                         user and assistant messages).\n",
    "                         \n",
    "    Returns:\n",
    "        None: The function prints the generated response in real-time and clears resources upon completion.\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    streamer = None\n",
    "    inputs = None\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer from Hugging Face\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Convert messages to model input format\n",
    "        print(\"Tokenizing input messages...\")\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Set up the TextStreamer to stream tokens as they are generated\n",
    "        print(\"Setting up streamer for real-time output...\")\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"Generating response...\")\n",
    "        model.generate(inputs['input_ids'], max_new_tokens=50, streamer=streamer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up resources manually if the function fails\n",
    "        if model is not None:\n",
    "            del model  # Delete the model\n",
    "            print(\"Model deleted\")\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            del tokenizer  # Delete the tokenizer\n",
    "            print(\"Tokenizer deleted\")\n",
    "\n",
    "        if inputs is not None:\n",
    "            del inputs  # Delete the inputs\n",
    "            print(\"Inputs deleted\")\n",
    "\n",
    "        if streamer is not None:\n",
    "            del streamer  # Delete the streamer\n",
    "            print(\"Streamer deleted\")\n",
    "\n",
    "        # Clear GPU memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_cpu(model_name: str, messages: list):\n",
    "    \"\"\"\n",
    "    Generate a response using a conversational model from Hugging Face, stream the response in real-time, \n",
    "    and clean up resources after execution.\n",
    "\n",
    "    This function loads a pre-trained language model and tokenizer from Hugging Face, tokenizes the input \n",
    "    messages, and streams the generated response token-by-token. After the response is generated (or if an \n",
    "    error occurs), the resources such as model, tokenizer, inputs, and streamer are cleaned up manually to \n",
    "    free memory.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name or path of the pre-trained model to use from Hugging Face.\n",
    "        messages (list): A list of messages to provide as input to the model. Each message is expected to \n",
    "                         follow the format required for conversational models (e.g., a list of alternating \n",
    "                         user and assistant messages).\n",
    "                         \n",
    "    Returns:\n",
    "        None: The function prints the generated response in real-time and clears resources upon completion.\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    streamer = None\n",
    "    inputs = None\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer from Hugging Face\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")  # Ensure CPU is used\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Convert messages to model input format\n",
    "        print(\"Tokenizing input messages...\")\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")  # No need for .to(\"cuda\")\n",
    "\n",
    "        # Set up the TextStreamer to stream tokens as they are generated\n",
    "        print(\"Setting up streamer for real-time output...\")\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"Generating response...\")\n",
    "        model.generate(inputs['input_ids'], max_new_tokens=50, streamer=streamer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up resources manually if the function fails\n",
    "        if model is not None:\n",
    "            del model  # Delete the model\n",
    "            print(\"Model deleted\")\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            del tokenizer  # Delete the tokenizer\n",
    "            print(\"Tokenizer deleted\")\n",
    "\n",
    "        if inputs is not None:\n",
    "            del inputs  # Delete the inputs\n",
    "            print(\"Inputs deleted\")\n",
    "\n",
    "        if streamer is not None:\n",
    "            del streamer  # Delete the streamer\n",
    "            print(\"Streamer deleted\")\n",
    "\n",
    "        # Clear memory cache for CPU\n",
    "        torch.cuda.empty_cache()  # Optional: Even on CPU, you can clear GPU memory cache, but it won't affect CPU usage.\n",
    "        print(\"Memory cache cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_responses_inference(model_name: str, messages: list):\n",
    "    \"\"\"\n",
    "    Generate a response using Hugging Face's free hosted inference API for warm conversational models.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name or path of the pre-trained model to use from Hugging Face.\n",
    "        messages (list): A list of messages to provide as input to the model.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the generated response in real-time.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Hugging Face API URL for model inference\n",
    "        inference_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "\n",
    "        # Prepare conversation input (convert list of messages to a single string)\n",
    "        print(\"Preparing input conversation...\")\n",
    "        context = \"\\n\".join([f\"User: {msg['content']}\" for msg in messages])\n",
    "\n",
    "        hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "        # Prepare headers and payload for the request\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {hf_token}\"  # Replace with your Hugging Face API key\n",
    "        }\n",
    "        payload = {\n",
    "            \"inputs\": context\n",
    "        }\n",
    "\n",
    "        # Send request to Hugging Face's inference API\n",
    "        print(\"Generating response...\")\n",
    "        response = requests.post(inference_url, headers=headers, json=payload)\n",
    "        \n",
    "        # Check for error responses\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error occurred: {response.status_code} {response.text}\")\n",
    "            return\n",
    "\n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "\n",
    "        # Check if the response contains the expected 'generated_text'\n",
    "        if isinstance(result, list) and 'generated_text' in result[0]:\n",
    "            generated_text = result[0]['generated_text']\n",
    "\n",
    "            # Print the full response but clean it up by removing the prompt\n",
    "            print(\"Response:\")\n",
    "            response_text = generated_text.replace(f\"User: {messages[-1]['content']}\", \"\").strip()\n",
    "            print(response_text)  # Only print the assistant's response, not the prompt\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Response doesn't contain the expected 'generated_text'. Full response:\")\n",
    "            print(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_huggingface_model_stream(model_name, messages, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Invokes a Hugging Face model via the Inference API and streams the response.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): The name of the warmed model on Hugging Face (e.g., 'gpt2', 'facebook/opt-1.3b').\n",
    "        messages (list of dict): Chat template messages, each a dict with `role` and `content` keys.\n",
    "        max_tokens (int): Maximum number of tokens to be retrieved in the response.\n",
    "\n",
    "    Returns:\n",
    "        None: Streams the response directly to the console or any specified streamer.\n",
    "    \"\"\"\n",
    "    inference_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    api_token = os.getenv(\"HF_TOKEN\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": messages,\n",
    "        \"parameters\": {\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stream\": True  # Enables streaming in the API\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    try:\n",
    "        # Open a stream connection\n",
    "        with requests.post(inference_url, headers=headers, json=payload, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            for chunk in response.iter_lines():\n",
    "                if chunk:  # Filter out keep-alive chunks\n",
    "                    decoded_chunk = json.loads(chunk.decode(\"utf-8\"))\n",
    "                    text = decoded_chunk.get(\"generated_text\", \"\")\n",
    "                    streamer.text(text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Loading Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quantized_model(model_directory):\n",
    "    \"\"\"\n",
    "    Load a quantized model and tokenizer from a specified directory.\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Generating Response from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, input_text, max_length=2000):\n",
    "    \"\"\"\n",
    "    Generate a response from the model and stream it.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for Streaming Response from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(input_text, quant_model_directory=\"quantized_model\"):\n",
    "    \"\"\"\n",
    "    Stream response from the quantized model for better visual experience.\n",
    "    \"\"\"\n",
    "    model_directory = f\"{quant_model_path}/{quant_model_directory}\"\n",
    "    model, tokenizer = load_quantized_model(model_directory)\n",
    "    response = generate_response(model, tokenizer, input_text)\n",
    "    \n",
    "    # Simulating a streaming response:\n",
    "    sys.stdout = StringIO()  # Capture printed output to simulate streaming\n",
    "    for char in response:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.1)  # Simulating stream delay\n",
    "    return sys.stdout.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(sum(9,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
